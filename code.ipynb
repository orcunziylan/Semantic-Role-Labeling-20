{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":697,"status":"ok","timestamp":1611667789858,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"g3cniGf5QKEu","outputId":"a2bf9b26-2957-401d-d8cb-f1da83402365"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1471,"status":"ok","timestamp":1611667790641,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"UotLArtA35Jj","outputId":"7bb52cd2-8577-4de6-8913-e0bd1a7e178b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Dersler/HWs/NLP/HW2\n","\u001b[0m\u001b[01;34mDataset\u001b[0m/  \u001b[01;34mGlove\u001b[0m/  HW2.ipynb  \u001b[01;34mnlp2020-hw2\u001b[0m/  \u001b[01;34mSaved\u001b[0m/\n"]}],"source":["%cd drive/MyDrive/Dersler/HWs/NLP/HW2/\n","%ls"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1804,"status":"ok","timestamp":1611667790982,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"DqIqowobu_y8"},"outputs":[],"source":["from tqdm.auto import tqdm\n","import torch"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4098,"status":"ok","timestamp":1611667793283,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"HNxnrYnllba4"},"outputs":[],"source":["import json, os\n","json_data = \"dev\"\n","\n","with open('Dataset/train.json', 'r') as json_file:\n","    train_raw = json.load(json_file)\n","with open('Dataset/dev.json', 'r') as json_file:\n","    dev_raw = json.load(json_file)\n","with open('Dataset/test.json', 'r') as json_file:\n","    test_raw = json.load(json_file)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4089,"status":"ok","timestamp":1611667793284,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"wP6NiUxuqey1","outputId":"2c0ac18d-65f0-4f41-9490-9a343e8a969e"},"outputs":[{"name":"stdout","output_type":"stream","text":["39279\n","words\n"," ['In', 'an', 'Oct.', '19', 'review', 'of', '``', 'The', 'Misanthrope', \"''\", 'at', 'Chicago', \"'s\", 'Goodman', 'Theatre', '(', '``', 'Revitalized', 'Classics', 'Take', 'the', 'Stage', 'in', 'Windy', 'City', ',', \"''\", 'Leisure', '&', 'Arts', ')', ',', 'the', 'role', 'of', 'Celimene', ',', 'played', 'by', 'Kim', 'Cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'Christina', 'Haag', '.']\n","lemmas\n"," ['in', 'an', 'oct.', '19', 'review', 'of', '``', 'the', 'misanthrope', \"''\", 'at', 'chicago', \"'s\", 'goodman', 'theatre', '-lrb-', '``', 'revitalize', 'classics', 'take', 'the', 'stage', 'in', 'windy', 'city', ',', \"''\", 'leisure', '&', 'arts', '-rrb-', ',', 'the', 'role', 'of', 'celimene', ',', 'play', 'by', 'kim', 'cattrall', ',', 'be', 'mistakenly', 'attribute', 'to', 'christina', 'haag', '.']\n","pos_tags\n"," ['IN', 'DT', 'NNP', 'CD', 'NN', 'IN', '``', 'DT', 'NN', \"''\", 'IN', 'NNP', 'POS', 'NNP', 'NNP', '(', '``', 'VBN', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'NNP', ',', \"''\", 'NNP', 'CC', 'NNS', ')', ',', 'DT', 'NN', 'IN', 'NNP', ',', 'VBN', 'IN', 'NNP', 'NNP', ',', 'VBD', 'RB', 'VBN', 'TO', 'NNP', 'NNP', '.']\n","dependency_heads\n"," ['43', '5', '4', '5', '1', '5', '9', '9', '6', '9', '9', '15', '12', '15', '11', '20', '20', '19', '20', '5', '22', '20', '20', '25', '23', '20', '20', '30', '30', '20', '20', '43', '34', '43', '34', '35', '34', '34', '38', '41', '39', '34', '0', '45', '43', '45', '48', '46', '43']\n","dependency_relations\n"," ['LOC', 'NMOD', 'NMOD', 'NMOD', 'PMOD', 'NMOD', 'P', 'NMOD', 'PMOD', 'P', 'LOC', 'NMOD', 'SUFFIX', 'NAME', 'PMOD', 'P', 'P', 'NMOD', 'SBJ', 'PRN', 'NMOD', 'OBJ', 'LOC', 'NAME', 'PMOD', 'P', 'P', 'NAME', 'NAME', 'TMP', 'P', 'P', 'NMOD', 'SBJ', 'NMOD', 'PMOD', 'P', 'APPO', 'LGS', 'NAME', 'PMOD', 'P', 'ROOT', 'MNR', 'VC', 'ADV', 'NAME', 'PMOD', 'P']\n","predicates\n"," ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'AROUSE_WAKE_ENLIVEN', '_', 'TAKE', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'PERFORM', '_', '_', '_', '_', '_', '_', 'ASCRIBE', '_', '_', '_', '_']\n","roles\n"," {'17': ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Experiencer', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], '19': ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Agent', '_', '_', 'Theme', 'Location', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], '37': ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Theme', '_', '_', '_', '_', 'Agent', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], '44': ['Location', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Attribute', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Attribute', '_', 'Theme', '_', '_', '_']}\n"]}],"source":["print(len(train_raw))\n","print(\"words\\n\", train_raw[\"0\"][\"words\"]) # input\n","print(\"lemmas\\n\", train_raw[\"0\"][\"lemmas\"]) # alternative input\n","print(\"pos_tags\\n\", train_raw[\"0\"][\"pos_tags\"])\n","print(\"dependency_heads\\n\", train_raw[\"0\"][\"dependency_heads\"])\n","print(\"dependency_relations\\n\", train_raw[\"0\"][\"dependency_relations\"])\n","print(\"predicates\\n\", train_raw[\"0\"][\"predicates\"])\n","print(\"roles\\n\", train_raw[\"0\"][\"roles\"]) # Find this"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1800,"status":"ok","timestamp":1611672216613,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"-wJsSMTLGBoP","outputId":"21b374a7-db32-48fa-c208-7bbae9022dc9"},"outputs":[],"source":["word2id = {}\n","id2word = {}\n","\n","word2id[\"<PAD>\"] = 0\n","id2word[0] = \"<PAD>\"\n","word2id[\"<UNK>\"] = 1\n","id2word[1] = \"<UNK>\"\n","word2id[\"<VERB>\"] = 2\n","id2word[2] = \"<VERB>\"\n","\n","def tokenizer(data, name):\n","  count=len(word2id)\n","  for lines in data:\n","    for word in data[lines][name]:\n","      if word != \"_\":\n","        \n","        try:\n","          id = word2id[word]\n","          # id2word[id] = word\n","        except:\n","          word2id[word] = count\n","          id2word[count] = word\n","          count += 1\n","\n","typeOfInput = \"words\" # words or lemmas\n","\n","tokenizer(train_raw, typeOfInput)\n","# tokenizer(dev_raw, typeOfInput)\n","tokenizer(train_raw, \"predicates\")\n","# tokenizer(dev_raw, \"predicates\")\n","\n","print(\"Vocabulary size:\", len(word2id))\n","print(\"Word to id:\\n\",word2id)\n","print(\"Word id to word:\\n\",id2word)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":2267,"status":"error","timestamp":1611672217094,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"EYc6LHv3uf7X","outputId":"3d55d305-4579-44af-947d-8d7954cbf9f4"},"outputs":[],"source":["def indexer(data, name):\n","  count = 0\n","  unk_count = 0\n","  indexed_sentences = []\n","  for lines in data:\n","    \n","    for NoOfRoles in range(len(data[lines][\"roles\"])):\n","      verb_count = 0\n","      indexed_words = []\n","      for word_no in range(len(data[lines][name])):\n","\n","        predicate = data[lines][\"predicates\"][word_no]\n","        word = data[lines][name][word_no]\n","        \n","        verb_flag = False\n","        if predicate != \"_\":\n","          verb_count += 1\n","          if verb_count == NoOfRoles+1:\n","            id = word2id[predicate]\n","            # id = word2id[\"<VERB>\"]\n","            verb_flag = True\n","          \n","        if not verb_flag:\n","          try:\n","            id = word2id[word]\n","          except:\n","            unk_count += 1\n","            id = word2id[\"<UNK>\"]\n","\n","        indexed_words.append(id)\n","      indexed_sentences.append(indexed_words)\n","\n","  if unk_count>0:\n","    print(\"Number of unknown word:\", unk_count)\n","\n","  return indexed_sentences\n","\n","train_indexed_x = indexer(train_raw, typeOfInput)\n","dev_indexed_x = indexer(dev_raw, typeOfInput)\n","print(\"\\nTest dataset,\")\n","test_indexed_x = indexer(test_raw, typeOfInput)\n","\n","print(\"\\nExample 'word index' - 'word' pairs\")\n","for word in train_indexed_x[0]:\n","  print(word, \"-\", id2word[word])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2249,"status":"aborted","timestamp":1611672217082,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"IPehCn80DrX6"},"outputs":[],"source":["label2id = {}\n","id2label = {}\n","label2id[\"<PAD>\"] = 0\n","id2label[0] = \"<PAD>\"\n","\n","def label_vocab(data, name):\n","  for lines in data:\n","    for role_set in data[lines][name]:\n","      for role in data[lines][name][role_set]:\n","        try:\n","          count=len(label2id)\n","          id = label2id[role]\n","          id2label[id] = role\n","        except:\n","          label2id[role] = count\n","          id2label[count] = role\n","          count += 1\n","\n","label_vocab(train_raw, \"roles\")\n","print(\"Number of label:\", len(label2id))\n","\n","print(\"Label to id:\", label2id)\n","print(\"Id to label:\", id2label)\n","\n","def label_indexer(data, max_length=0):\n","  count = 0\n","  unk_count = 0\n","  indexed_sentences = []\n","  for lines in data:\n","    for set_roles in data[lines][\"roles\"]:\n","      indexed_words = []\n","\n","      for role in data[lines][\"roles\"][set_roles]:        \n","        id = label2id[role]\n","        indexed_words.append(id)\n","      indexed_sentences.append(indexed_words)\n","\n","  if unk_count>0:\n","    print(\"\\nNumber of unknown label:\", unk_count)\n","\n","  return indexed_sentences\n","\n","train_indexed_y = label_indexer(train_raw)\n","dev_indexed_y = label_indexer(dev_raw)\n","test_indexed_y = label_indexer(test_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2238,"status":"aborted","timestamp":1611672217083,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"rg7TLGYAIThf"},"outputs":[],"source":["for lines in train_raw:\n","  for set_roles in train_raw[lines][\"roles\"]:\n","      for role in train_raw[lines][\"roles\"][set_roles]:\n","        print(role)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2224,"status":"aborted","timestamp":1611672217084,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"k0eEqDUQm-pw"},"outputs":[],"source":["a = len(max(train_indexed_y, key=len))\n","b = len(max(dev_indexed_y, key=len))\n","c = len(max(test_indexed_y, key=len))\n","max_length = max(a, b, c)\n","print(\"Max sentence length =\", max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2217,"status":"aborted","timestamp":1611672217084,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"XxbxtLLCnZDh"},"outputs":[],"source":["def padding_opt(dataset, max_length=max_length):\n","  for line in dataset:\n","    while len(line) < max_length:\n","      line.append(word2id[\"<PAD>\"])\n","  return dataset\n","\n","train_padded_x = padding_opt(train_indexed_x)\n","train_padded_y = padding_opt(train_indexed_y)\n","\n","dev_padded_x = padding_opt(dev_indexed_x)\n","dev_padded_y = padding_opt(dev_indexed_y)\n","\n","test_padded_x = padding_opt(test_indexed_x)\n","test_padded_y = padding_opt(test_indexed_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2214,"status":"aborted","timestamp":1611672217085,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"so7dHm-NhAVy"},"outputs":[],"source":["def build_dataset(inputs, outputs, device=\"cuda\"):\n","  dataset = []\n","  for i in tqdm(range(len(inputs))):\n","    x = inputs[i]\n","    y = outputs[i]\n","    \n","    element = {\"inputs\": torch.LongTensor(x).to(device), \"outputs\": torch.LongTensor(y).to(device)}\n","    dataset.append(element)\n","  return dataset\n","\n","train_dataset = build_dataset(train_padded_x, train_padded_y)\n","valid_dataset = build_dataset(dev_padded_x, dev_padded_y)\n","test_dataset = build_dataset(test_padded_x, test_padded_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2205,"status":"aborted","timestamp":1611672217085,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"5qk0uzFJjx9V"},"outputs":[],"source":["import time\n","import csv\n","from tqdm.auto import tqdm\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","import seaborn as sn\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2201,"status":"aborted","timestamp":1611672217085,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"dVb_dkodl0ru"},"outputs":[],"source":["train_dataset = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","valid_dataset = DataLoader(valid_dataset, batch_size=32)\n","test_dataset = DataLoader(test_dataset, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2198,"status":"aborted","timestamp":1611672217086,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"vJL1ce01G6fl"},"outputs":[],"source":["%cd /content/drive/My Drive/HWs/NLP/HW2/\n","embeddings = {}\n","path = path = \"Glove/\"\n","name = \"glove.6B.100d.txt\"\n","with open(path + name, 'r') as f:\n","  for line in f:\n","    values = line.split()\n","    word = values[0] # first element is the word\n","    vector = np.asarray(values[1:], \"float32\") # rest is vector elements\n","    embeddings[word] = vector\n","\n","pre_trained_emb = []\n","\n","\n","for word in word2id:\n","  \n","  try:\n","    x = embeddings[word]  \n","    \n","  except:\n","    x = np.random.rand(100)\n","\n","  pre_trained_emb.append(torch.LongTensor(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2191,"status":"aborted","timestamp":1611672217086,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"OussKyB9tnJT"},"outputs":[],"source":["class SimpleModel(nn.Module):\n","    def __init__(self, vocab_length=len(word2id), embedding=pre_trained_emb, num=2, dropout=0.5):\n","        super(SimpleModel, self).__init__()\n","        self.word_embedding = nn.Embedding(vocab_length, 100, padding_idx=word2id['<PAD>']) # vocab size, embedding dimension size\n","        if embedding:\n","          print(\"pretrained embedding!\")\n","          self.word_embedding.weight.data.copy_(torch.stack(embedding))\n","\n","        self.lstm = nn.LSTM(100, # embedding dimension\n","                            100, # hidden dimension\n","                            bidirectional=True, \n","                            num_layers=num, # number of lstm layer\n","                            dropout = dropout, \n","                            batch_first=True)\n","        \n","        lstm_output_dim = 200 # \"hidden dimension x 2\" for bidirectional\n","        self.dropout = nn.Dropout(dropout)\n","        self.classifier = nn.Linear(lstm_output_dim, len(label2id)) # lstm output dimension, number of the labels in the dataset\n","\n","    def forward(self, x):\n","        embeddings = self.word_embedding(x)\n","        embeddings = self.dropout(embeddings)\n","        o, (h, c) = self.lstm(embeddings)\n","        o = self.dropout(o)\n","        output = self.classifier(o)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2188,"status":"aborted","timestamp":1611672217087,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"n-f-pfxAvqpF"},"outputs":[],"source":["class Trainer():\n","  def __init__(self, model: nn.Module, loss_function, optimizer, \n","               label_vocab: label2id):\n","    \n","    self.model = model\n","    self.loss_function = loss_function\n","    self.optimizer = optimizer\n","\n","    self.label_vocab = label_vocab\n","        \n","  def train(self, train_dataset, \n","            valid_dataset, \n","            epochs:int=1):\n","\n","    assert epochs > 0 and isinstance(epochs, int)\n","\n","    train_loss = 0.0\n","    max_f1 = 0\n","    for epoch in range(epochs):\n","      \n","      epoch_loss = 0.0\n","      self.model.train()\n","      print(\"Epoch:\", epoch+1, \"/\", epochs)\n","      for step, data in enumerate(tqdm(train_dataset)):    \n","        self.optimizer.zero_grad()\n","\n","        inputs = data['inputs']\n","        labels = data['outputs']\n","\n","        preds = self.model(inputs)\n","        preds = preds.view(-1, preds.shape[-1])\n","        labels = labels.view(-1)\n","        \n","        sample_loss = self.loss_function(preds, labels)\n","        sample_loss.backward()\n","        self.optimizer.step()\n","\n","        epoch_loss += sample_loss.tolist()\n","\n","      avg_epoch_loss = epoch_loss / len(train_dataset)\n","      train_loss += avg_epoch_loss\n","      valid_loss = self.evaluate(valid_dataset)\n","\n","      print(\"Train loss = {:0.4f}, Valid loss = {:0.4f}\".format(avg_epoch_loss, valid_loss))\n","\n","      f1 = get_f1(simple, test=test_dataset)\n","\n","      if max_f1 < f1:\n","        max_f1 = f1\n","        torch.save(self.model.state_dict(), \"Saved/\" + str(f1))\n","        print(\"Model saved!,\",max_f1)\n","\n","      loss_x.append(avg_epoch_loss)\n","      loss_y.append(valid_loss)\n","\n","    avg_epoch_loss = train_loss / epochs\n","     \n","    \n","    return avg_epoch_loss\n","\n","\n","  def evaluate(self, valid_dataset):\n","\n","    valid_loss = 0.0\n","    self.model.eval()\n","    with torch.no_grad():\n","      for data in valid_dataset:\n","        inputs = data['inputs']\n","        labels = data['outputs']\n","\n","        preds = self.model(inputs)\n","        preds = preds.view(-1, preds.shape[-1])\n","        labels = labels.view(-1)\n","        sample_loss = self.loss_function(preds, labels)\n","        valid_loss += sample_loss.tolist()\n","    \n","    return valid_loss / len(valid_dataset)\n","\n","  def predict(self, x):\n","\n","    self.model.eval()\n","    with torch.no_grad():\n","      logs = self.model(x)\n","      preds = torch.argmax(logs, -1)\n","      return logs, preds"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2184,"status":"aborted","timestamp":1611672217087,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"RynLZZjTPW_J"},"outputs":[],"source":["def get_f1(model:nn.Module, test:DataLoader, get_preds= False):\n","    predictions = list()\n","    labels = list()\n","    for data in test:\n","        x = data[\"inputs\"]\n","        pred = model(x)\n","        pred = torch.argmax(pred, -1).view(-1)\n","\n","        y = data[\"outputs\"]\n","        label = y.view(-1)\n","        valid_indices = label != 0\n","        \n","        valid_predictions = pred[valid_indices]\n","        valid_labels = label[valid_indices]\n","        \n","        predictions.extend(valid_predictions.tolist())\n","        labels.extend(valid_labels.tolist())\n","        \n","    id_precision, id_recall, id_f1 = argument_identification(predictions, labels)\n","    cl_precision, cl_recall, cl_f1 = argument_classification(predictions, labels)\n","\n","    # print(\"Argument Identification - f1 score:\", id_f1)\n","    # print(\"Argument disambiguation - f1 score:\", cl_f1)\n","    print(\"Argument Identification - f1 score = {:0.4f}\\nArgument Disambiguation - f1 score = {:0.4f}\\n\".format(id_f1, cl_f1))\n","    if get_preds:\n","      return predictions, labels, cl_f1\n","    \n","    return cl_f1\n","\n","def argument_identification(pred, gold):\n","  tp, fp, fn = 0, 0, 0\n","  excluded = label2id[\"_\"]\n","  for p, g in zip(pred, gold):    \n","    if p != excluded and g != excluded:\n","      tp += 1\n","    elif p != excluded and g == excluded:\n","      fp += 1\n","    elif p == excluded and g != excluded:\n","      fn += 1\n","  precision = tp / (tp + fp)\n","  recall = tp / (tp + fn)\n","  f1 = 2 * (precision * recall) / (precision + recall)\n","  return precision, recall, f1\n","\n","def argument_classification(pred, gold):\n","  tp, fp, fn = 0, 0, 0\n","  excluded = label2id[\"_\"]\n","  for p, g in zip(pred, gold):\n","    if p != excluded and g != excluded:      \n","      if p == g:\n","        tp += 1\n","      else:\n","        fp += 1\n","        fn += 1\n","    elif p != excluded and g == excluded:\n","      fp += 1\n","    elif p == excluded and g != excluded:\n","      fn += 1\n","  precision = tp / (tp + fp)\n","  recall = tp / (tp + fn)\n","  f1 = 2 * (precision * recall) / (precision + recall)\n","  return precision, recall, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2182,"status":"aborted","timestamp":1611672217088,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"MUFY5kZKJeoy"},"outputs":[],"source":["# Save:\n","\n","# torch.save(model.state_dict(), PATH)\n","# Load:\n","\n","# model = TheModelClass(*args, **kwargs)\n","# model.load_state_dict(torch.load(PATH))\n","# model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2177,"status":"aborted","timestamp":1611672217088,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"fxP_NX3IPyOP"},"outputs":[],"source":["simple = SimpleModel().cuda()\n","simple_trainer = Trainer( \n","  model=simple,\n","  loss_function=nn.CrossEntropyLoss(ignore_index=word2id['<PAD>']),\n","  optimizer=optim.Adam(simple.parameters(), lr=0.003),\n","  label_vocab=label2id\n",")\n","\n","loss_x = []\n","loss_y = []\n","simple_trainer.train(train_dataset, valid_dataset, 20)\n","\n","plt.plot(loss_x, \"-b\", loss_y , \"-r\")\n","print(\"Testing on test dataset,\\n\")\n","pred_y, gold_y, _ = get_f1(simple, test_dataset, get_preds=True)\n","data = confusion_matrix(pred_y, gold_y)\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2168,"status":"aborted","timestamp":1611672217088,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"K73o07CXDu3U"},"outputs":[],"source":["simple.eval()\n","simple(encoded_a)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2157,"status":"aborted","timestamp":1611672217090,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"Qvw_XW1NTohs"},"outputs":[],"source":["def encode(data, vocab, device=\"cuda\", max=max_length):\n","  length_sentences = []\n","  encoded_sentences = []\n","  for sentence in data:    \n","    encoded_words = []\n","    for word in sentence:\n","      try:\n","        element = vocab[word]\n","      except:\n","        element = vocab[\"<UNK>\"]\n","      encoded_words.append(element)\n","    length_sentences.append(len(sentence))\n","    while len(encoded_words) < max:\n","      encoded_words.append(vocab[\"<PAD>\"])\n","    encoded_sentences.append(torch.LongTensor(encoded_words).to(device))\n","  return torch.stack(encoded_sentences), length_sentences\n","\n","def decode(data, labels, length):\n","  decoded_sequences = []\n","  for seq_index in range(len(data)):\n","    decoded_labels = []\n","    for word_lengths in range(length[seq_index]):\n","      sequence = data[seq_index]\n","      label_id = sequence[word_lengths]\n","      for key in labels.keys():\n","        if label_id == labels[key]:\n","          decoded_labels.append(key)\n","    decoded_sequences.append(decoded_labels)\n","  return decoded_sequences\n","\n","a = [[\"The\", \"cat\", \"ate\", \"the\", \"fish\", \".\"]]\n","\n","encoded_a, lengths = encode(a, word2id)\n","logits = model(encoded_a)\n","predictions = torch.argmax(logits, -1)\n","print(predictions)\n","pred = decode(predictions, id2label, lengths)\n","print(pred)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2155,"status":"aborted","timestamp":1611672217091,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"JvYohrA3EnzH"},"outputs":[],"source":["for x_id in range(len(pred[0])):\n","  print(a[0][x_id],\":\", pred[0][x_id])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2148,"status":"aborted","timestamp":1611672217091,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"7EKYccXFDNnp"},"outputs":[],"source":["\n","pred_y, gold_y, _ = get_f1(simple, encoded_a, get_preds=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2135,"status":"aborted","timestamp":1611672217092,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"ZLStViA-DuGE"},"outputs":[],"source":["for x_id in range(len(pred[0])):\n","  print(a[0][x_id],\":\", pred[0][x_id])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2133,"status":"aborted","timestamp":1611672217092,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"fe7LdILwOXzQ"},"outputs":[],"source":["cm_hm = pd.DataFrame(data, columns=np.unique(gold_y), index = np.unique(gold_y))\n","cm_hm.index.name = 'Actual'\n","cm_hm.columns.name = 'Predicted'\n","plt.figure(figsize = (20,14))\n","\n","sn.heatmap(cm_hm, cmap=\"gist_heat_r\", annot=True, annot_kws={\"size\": 16}, fmt=\"d\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2124,"status":"aborted","timestamp":1611672217093,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"2ef329vz-q7V"},"outputs":[],"source":["path = \"Saved/0.801840490797546\"\n","model = SimpleModel().cuda()\n","model.load_state_dict(torch.load(path))\n","model.eval()\n","f1 = get_f1(model, test_dataset)\n","print(model)\n","print(\"The model I used, f1:\", f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2119,"status":"aborted","timestamp":1611672217094,"user":{"displayName":"orcun ziylan","photoUrl":"","userId":"10450429657357457032"},"user_tz":-180},"id":"sg5mUiJldq2y"},"outputs":[],"source":["pred_y, gold_y, _ = get_f1(model, test_dataset, get_preds=True)\n","encoded_a, lengths = encode(a, word2id)\n","logits = model(encoded_a)\n","predictions = torch.argmax(logits, -1)\n","print(predictions)\n","pred = decode(predictions, id2label, lengths)\n","print(pred)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"HW2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
